<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=yes">
    <meta name="description" content="Swapnil Kumar, ME IITB's personal website and info">
    <meta name="author" content="Swapnil Kumar">
    <meta name="keywords" content="Swapnil Kumar ME IITB Mechanical Engineering Indian Institute of Technology Bombay Resume Bio About UG Engineering Home Page ">

    <title>Swapnil Kumar</title>

    <link rel="stylesheet" type="text/css" href="./css/style.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"> 
  </head>
  
  <body>
    <br>
    <div class="intro_right">
      <img class="profile_img" src="./img/profile.jpeg" alt="Profile picture" style="height:auto; border-radius:50%; display: block;margin: auto;">
      <p align="center">
        <a href="mailto:jkswapnil99@gmail.com" style="margin-right:20px" target="_blank">
            <i class="fa fa-envelope" style="font-size:40px"></i></a>

        <a href ="https://github.com/jkSwapnil" style="margin-right:20px" target="_blank">
            <i class="fa fa-github" style="font-size:40px"></i></a>

        <a href ="https://www.linkedin.com/in/swapnil-kumar-902451139/" style="margin-right:20px" target="_blank">
            <i class="fa fa-linkedin" style="font-size:40px"></i></a>

        <a href ="https://jkswapnil.github.io" target="_blank">
            <i class="fa fa-globe" style="font-size:40px"></i></a>
      </p>
    </div>

    <div class="intro_left">
      <p align="center"><name>Swapnil Kumar</name></p>
      <p>I am a senior undergraduate at <a href="http://www.iitb.ac.in/" target="_blank">IIT Bombay</a> pursuing a major in Mechanical Engineering and a minor in Computer Science and Engineering. My interest lies in areas of Artificial Intelligence with a focus on Computer Vision.  I'm also interested in distributed computing, numerical simulations, and robotics. I'm an emeritus member of the <a href ="https://iitbmartian.github.io/" target="_blank">IITB Mars Rover Team</a>. Currently, I'm working on developing an efficient, modular, and versatile system for the Human Activity Recognition in the Machine Intelligence Program under the supervision of <a href="https://www.me.iitb.ac.in/?q=faculty/Prof.%20Asim%20Tewari" target="_blank">Prof. Asim Tewari</a>. Please visit my <a href="https://jkswapnil.github.io" target="_blank">webpage</a> to know further about my project works and interests.
      </p>
    </div>

    <br>

    <heading>Research Work</heading>
    <p> 
    <sr>1.</sr><subheading> Objective and Motivation</subheading>
    <br>

    Human activity recognition is concerned with developing intelligent agents that can perform the recognition tasks on par with the human brain. It finds applications in interactive healthcare, autonomous surveillance systems, entertainment, autonomous driving, computer graphics, and many more. The classical approaches used handcrafted features (holistic and local) to represent and classify the action. This required domain expertise and also did not generalize well. Only recently, the problem has been addressed by deep learning-based methods such as 3D-CNN, multi-stream networks, hybrid-networks, etc. Instead of relying on the pre-determined set of features, these models hierarchically learn them to represent the action. Due to this, there have been great improvements in the state-of-the-art accuracy of the recognition systems. But, many previous works developed deep-networks that were end-to-end trainable. Very deep networks are hard to train and are computationally intensive. Also, complete retraining of the model is required for moving to a different dataset. If transfer learning is used, it will lead to even deeper networks. Hence, the objective of this project is to develop a versatile, efficient, and modular method for activity recognition. A new 2-Stage approach is proposed with separate pre-processing and classification modules. It can be easily modified, trained, and gives a competitive performance to the state-of-the-art.</p>

    <p>
    <sr>2.</sr><subheading> Approach and Results</subheading>
    <br>
    </p>

    <img class="figure" src="./img/2-stage_model.png" alt="Image to show complete pipeline of our model">

    <p>The figure above gives the complete pipeline of our model. In this project, we have limited ourselves to the 2D domain. The model takes input in the form of monocular RGB videos. The first stage uses a pose estimation algorithm to localize the crucial human joints responsible for the pose. This step is performed on every frame of the input video and localized joint coordinates are used to evaluate joint angles and displacements. These values are then further concatenated to create an input vector, which encodes the spatial information of the action at every timestamp (frame). In the second stage, a shallow recurrent network classifies the action by studying input vectors of a fixed number of consecutive frames. For training and testing the model, we have used the <a href="https://www.csc.kth.se/cvap/actions/" target="_blank">KTH</a> dataset, which is a standard and extensively used datasets in the field of activity recognition.</p>

    <div style="overflow-x:scroll;">
    <table>
	  <tr>
    	<th>Method</th>
    	<th>Boxing</th>
    	<th>Handclapping</th>
    	<th>Handwaving</th>
    	<th>Jogging</th>
    	<th>Running</th>
    	<th>Walking</th>
    	<th>Overall</th>
      </tr>
      <tr>
    	<th>2-Stage</th>
    	<td>99.99</td>
    	<td>99.96</td>
    	<td>99.85</td>
    	<td>94.97</td>
    	<td>92.71</td>
    	<td>91.79</td>
    	<td>99.51</td>
      </tr>
      <tr>
    	<th><a href="https://ieeexplore.ieee.org/document/6165309" target="_blank">3D-CNN</a></th>
    	<td>90</td>
    	<td>94</td>
    	<td>97</td>
    	<td>84</td>
    	<td>79</td>
    	<td>97</td>
    	<td>90.2</td>
      </tr>
      <tr>
    	<th>
    		<a href="https://www.researchgate.net/publication/4090526_Recognizing_human_actions_A_local_SVM_approach" target="_blank">Schuldt</a>
    	</th>
    	<td>97.9</td>
    	<td>59.7</td>
    	<td>73.6</td>
    	<td>60.4</td>
    	<td>54.9</td>
    	<td>83.8</td>
    	<td>71.7</td>
      </tr>
      <tr>
    	<th>
    		<a href="https://www.researchgate.net/publication/4210181_Behavior_recognition_via_sparse_spatio-temporal_features" target="_blank">Dollar</a>
    	</th>
    	<td>93</td>
    	<td>77</td>
    	<td>85</td>
    	<td>57</td>
    	<td>85</td>
    	<td>90</td>
    	<td>81.2</td>
      </tr>
      <tr>
    	<th><a href="https://link.springer.com/article/10.1007/s11263-007-0122-4" target="_blank">Niebles</a></th>
    	<td>98</td>
    	<td>86</td>
    	<td>93</td>
    	<td>53</td>
    	<td>88</td>
    	<td>82</td>
    	<td>83.3</td>
      </tr>
      <tr>
    	<th><a href="https://ieeexplore.ieee.org/document/4408988" target="_blank">Jhuang</a></th>
    	<td>92</td>
    	<td>98</td>
    	<td>92</td>
    	<td>85</td>
    	<td>87</td>
    	<td>96</td>
    	<td>91.7</td>
      </tr>
	</table>
	</div>

	<p>The table gives a quantitative comparison of the performance of our model to the other state-of-the-art. The 2-Stage method gives far better performance. Apart from this, we also performed a qualitative analysis of the model by print out the labels. This helps us to better visualize the misclassification. The video clips shown below are of the the <a href="https://www.csc.kth.se/cvap/actions/" target="_blank">KTH</a> dataset.</p>

    <div class="row">
    	<div class="column">
    		<video class="figure" controls>
      			<source src="./vid/boxing_output.mp4" type="video/mp4">
      			Your browser does not support the video tag.
    		</video>
    		<video class="figure" controls>
      			<source src="./vid/handclapping_output.mp4" type="video/mp4">
      			Your browser does not support the video tag.
    		</video>
		</div>
		<div class="column">
    		<video class="figure" controls>
      			<source src="./vid/handwaving_output.mp4" type="video/mp4">
      			Your browser does not support the video tag.
    		</video>
    		<video class="figure" controls>
      			<source src="./vid/running_output.mp4" type="video/mp4">
      			Your browser does not support the video tag.
    		</video>
		</div>
		<div class="column">
    		<video class="figure" controls>
      			<source src="./vid/walking_output.mp4" type="video/mp4">
      			Your browser does not support the video tag.
    		</video>
    		<video class="figure" controls>
      			<source src="./vid/jogging_output.mp4" type="video/mp4">
      			Your browser does not support the video tag.
    		</video>
		</div>
	</div>

    <p>Splitting the job into two stages introduces modularity in the model. Because stage one (joint localization) is independent of stage two (classification), we can switch between datasets by simply retraining the shallow classifier. Similarly, stage one can be modified independently of the classifier. For example, the pre-processing step in stage one can be tweaked to enable multiple-subject action recognition on the model trained on a single-subject action recognition dataset, as shown in the video clip below (concatenatedating two <a href="https://www.csc.kth.se/cvap/actions/" target="_blank">KTH</a> dataset videos). This depicts the versatile nature of the model due to its modularity.</p>

    <video class="figure" controls>
      <source src="./vid/2_subjects.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>

    <p> Due to its shallow architecture, the classifier is much easier to train as compared to other deep-networks. Thus, fulfilling our original objectives. We are also working on an implementation of the project in the form of a payment system for assembly lines and shop floors. If you wish to know further about the project please, feel free to contact me.</p>


    <p>
    <sr>3.</sr><subheading> Limitations and Future work</subheading>
    <br>
    Following are the issues being faced by the current model and the possible directions in which work can be done to solve them -
      <ul>
        <li>The model suffers from viewpoint changes as it operates in the 2D domain. We can get a much better representation of the action if we have the joint positions and angles in 3D. We are exploring the use of RGBD videos instead of the plain RGB or other sensor-based modes of input alongside to get the depth information.</li>
        <br>
        <li>Till now, we focus only on a specific person's pose variation for activity recognition. The current model cannot recognize group activities such as playing, discussions, fights, etc. We are actively working on including data of interactions with neighboring subjects and objects, to enhance the model's capability to recognize complex actions.</li>
      </ul>
    </p>
	<br>
</html>