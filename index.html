<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=yes">
    <meta name="description" content="Swapnil Kumar, ME IITB's personal website and info">
    <meta name="author" content="Swapnil Kumar">
    <meta name="keywords" content="Swapnil Kumar ME IITB Mechanical Engineering Indian Institute of Technology Bombay Resume Bio About UG Engineering Home Page ">

    <title>Swapnil Kumar</title>

    <link rel="stylesheet" type="text/css" href="./css/style.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"> 
  </head>
  
  <body>
    <br>
    <div class="intro_right">
      <img class="profile_img" src="./img/profile.jpeg" alt="Profile picture" style="height:auto; border-radius:50%; display: block;margin: auto;">
      <p align="center">
        <a href="mailto:jkswapnil99@gmail.com" style="margin-right:20px" target="_blank">
            <i class="fa fa-envelope" style="font-size:40px"></i></a>

        <a href ="https://github.com/jkSwapnil" style="margin-right:20px" target="_blank">
            <i class="fa fa-github" style="font-size:40px"></i></a>

        <a href ="https://www.linkedin.com/in/swapnil-kumar-902451139/" style="margin-right:20px" target="_blank">
            <i class="fa fa-linkedin" style="font-size:40px"></i></a>

        <a href ="https://jkswapnil.github.io" target="_blank">
            <i class="fa fa-globe" style="font-size:40px"></i></a>
      </p>
    </div>

    <div class="intro_left">
      <p align="center"><name>Swapnil Kumar</name></p>
      <p>I am a senior undergraduate student at <a href="http://www.iitb.ac.in/" target="_blank">IIT Bombay</a> pursuing a major in Mechanical Engineering and a minor in Computer Science and Engineering.      
      My interest lies in areas of Artificial Intelligence with a focus on Computer Vision, Numerical, and Robotic Simulations. I have undertaken various projects, courses, and other experiences in these fields. I am also fascinated by other fields, including distributed computing, robotics, and optimization.
      I'm a member <a href ="https://iitbmartian.github.io/" target="_blank">IITB Mars Rover Team</a> and I'm also working on developing an efficient, accurate and versatile Human Activity Recognition system in the Machine Intelligence Program under the supervision of <a href="https://www.me.iitb.ac.in/?q=faculty/Prof.%20Asim%20Tewari" target="_blank">Prof. Asim Tewari</a>.
      </p>
    </div>

    <br>

    <heading>Research Work</heading>
    <p> 
    <sr>1.</sr><subheading> Objective and Motivation</subheading>
    <br>
    Modeling and Recognition of Human Activities have been a hot research topic in recent times. It finds many practical applications like interactive healthcare, autonomous surveillance systems, entertainment, autonomous driving, computer graphics, etc. Classical approaches used handcrafted features (holistic and local) to model and classify the action. These required expertise and did not generalize well. Only in recent times, the problem has been addressed by deep learning-based methods such as 3D-CNN, Multi-stream networks, Hybrid-networks, etc. Instead of relying on the pre-determined set of features, these models hierarchically learn rich and domain-specific features. Due to the success of these deep-networks, there have been great improvements in the state-of-the-art accuracy of the recognition systems.<!-- </p> -->

   <!--  <p style="text-indent: 25px;"> -->
    Despite these huge leaps, most of the Human Activity Recognition systems are computationally intensive and require huge resources. Also, the models are domain-specific thus, changing the working domain of the model requires re-training (except for bottom layers if transfer-learning is used) on the new dataset. In this project, the objective is to develop a computationally efficient, modular, and versatile model for activity recognition. We have developed a 2-Stage skeleton-based approach for modeling action, using single vision-based input (monocular RGB videos). Our model is more efficient, can easily be extended to different domains, and gives competitive performance to the state-of-the-art.</p>

    <p>
    <sr>2.</sr><subheading> Approach and Results</subheading>
    <br>
    We proposed a unique 2-Stage skeleton-based approach to achieve the above goals. For now, we focused only on vision-based input in form of monocular RGB videos. The figure below shows the complete pipeline of our model.</p>

    <img class="figure" src="./img/2-stage_model.png" alt="Image to show complete pipeline of our model">

    <p>The first stage of the model uses a Human Pose Estimation algorithm to localize the crucial Human joints responsible for the Pose. This step is done for each frame of the input video and localized joint coordinates are to be used to evaluate joint angles. This is shown in the figure below. The action is classified by studying temporal variations on these poses (joint angles). The video clip below shows action recognition on the KTH dataset.</p>

    <video class="figure" controls>
      <source src="./vid/1_subject.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>

    <p>Splitting the job into two stages introduces modularity in the model. Because the joint localization stage is independent of the domain and application, the model can quickly be modified and trained on a different dataset by just working on the classifier stage. Similarly, the joint localization stage can be modified independently of the classifier. For example, the pre-processing stage of classifier (trained on single-person action recognition dataset) can be tweaked to enable multiple-person action recognition, without retraining, as shown in the video below. This depicts the versatile nature of the model due to its modularity.</p>

    <video class="figure" controls>
      <source src="./vid/2_subjects.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>

    <p> Our model is computationally efficient as compared to the state-of-the-art due to the shallow architecture of its' recurrent classifier. The following table shows a comparison of our model's accuracy on the KTH dataset with other state-of-the-art models. Our model gives better or comparable accuracies in addition to being efficient and versatile thus, fulfilling our original objective. We are also currently working on an implementation of the project in form of a payment system. If you wish to know further about the project please, feel free to contact me.</p>

    <img class="figure" src="./img/model_comparison.png" alt="Comparison of results on KTH datasets" style="width: 100%">

    <p>
    <sr>3.</sr><subheading> Limitations and Future work</subheading>
    <br>
    Following are the issues being faced by the current model and the possible directions in which work can be done to solve them -
      <ul>
        <li>The current model localizes and evaluates the angles in 2D-space. We can get a much better idea of pose variations if we know the localized joint positions and angles in a 3D-space. We are exploring using RGBD videos instead of the plain RGB or other sensor-based modes of input alongside to get the depth information.</li>
        <br>
        <li>As we focus only on a specific person's pose variation for activity recognition. The current iteration of the model cannot recognize group activities such as playing, discussions, fights, etc. We are actively working on the inclusion of a person's interactions with neighboring subjects and objects along with join angles. This will enable the model to recognize activities involving interaction between multiple subjects or a subject with an object.</li>
      </ul>
    </p>
	<br>
</html>